{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "dockerImageVersionId": 30786,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Required Libraries**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "FAISS for vector database,\n",
        "Beautifulsoup for web scraping,\n",
        "Gradio for UI,\n",
        "LLM Model = BLOOM,\n",
        "SMTP for email\n"
      ],
      "metadata": {
        "id": "rjM2FpYTln7s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install faiss-cpu"
      ],
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lvEp3dgkln76",
        "outputId": "795ce6ab-58d4-41c9-f31c-79a3c0b00f54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.10/dist-packages (1.9.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (24.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "from transformers import pipeline\n",
        "from bs4 import BeautifulSoup\n",
        "import smtplib\n",
        "from email.mime.multipart import MIMEMultipart\n",
        "from email.mime.text import MIMEText\n",
        "import requests\n",
        "import sqlite3\n",
        "import faiss\n",
        "import numpy as np"
      ],
      "metadata": {
        "trusted": true,
        "id": "C3gUNU3_ln8A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Using BLOOM Model**"
      ],
      "metadata": {
        "id": "do3alvmzMn2x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification,AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model_name = \"bigscience/bloom-560m\"\n",
        "# BLOOM Model\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name,device_map=\"cpu\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ],
      "metadata": {
        "trusted": true,
        "id": "ZGVEIcCaln8G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Chat Buffer initialization**"
      ],
      "metadata": {
        "id": "9C71szFpMxeN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#chat buffer memory initialization\n",
        "chat_buffer = []\n",
        "\n",
        "# generator initialization\n",
        "generator = pipeline('text-generation', model=model, tokenizer=tokenizer)\n",
        "\n",
        "# Response based on the user's input\n",
        "def generate_response(input_text):\n",
        "    # Generate a response using the BLOOM model\n",
        "    response = generator(input_text, max_length=200,truncation=True)\n",
        "    return response[0]['generated_text']"
      ],
      "metadata": {
        "trusted": true,
        "id": "N9oTIHNFln8J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Extract information from the internet using web scraping\n",
        "def extract_info(url):\n",
        "    # Send an HTTP request to the URL\n",
        "    response = requests.get(url)\n",
        "    # Parse the HTML content using BeautifulSoup\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    # Extract the text from the HTML content\n",
        "    # text = soup.get_text()\n",
        "    text = ' '.join([p.get_text() for p in soup.find_all('p')])\n",
        "\n",
        "    return text"
      ],
      "metadata": {
        "trusted": true,
        "id": "9tYw_dAcln8R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extract_info(\"https://www.analyticsvidhya.com/blog/2024/10/function-calling-in-ai-agents-using-mistral-7b/\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "id": "7OD2Zskiln8T",
        "outputId": "77785d6f-ea25-4ff0-d2f1-295b18bfcbe0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Mastering Python’s Set Difference: A Game-Changer for Data Wrangling Function calling in large language models (LLMs) has transformed how AI agents interact with external systems, APIs, or tools, enabling structured decision-making based on natural language prompts. By using JSON schema-defined functions, these models can autonomously select and execute external operations, offering new levels of automation. This article will demonstrate how function calling can be implemented using Mistral 7B, a state-of-the-art model designed for instruction-following tasks. This article was published as a part of the\\xa0Data Science Blogathon. In the scope of Generative AI (GenAI), AI agents represent a significant evolution in artificial intelligence capabilities. These agents use models, such as large language models (LLMs), to create content, simulate interactions, and perform complex tasks autonomously. The AI agents enhance their functionality and applicability across various domains, including customer support, education, and medical domain. They can be of several types (as shown in the figure below) including :\\xa0 Function Calling is the combination of Code execution,\\xa0 Tool execution, and Model Inference i.e. while the LLMs handle natural language understanding and generation, the Code Executor can execute any code snippets needed to fulfill user requests. We can also use the Humans in the loop, to get feedback during the process, or when to terminate the process. Developers define functions using JSON schemas (which are passed to the model), and the model generates the necessary arguments for these functions based on user prompts. For example: It can call weather APIs to provide real-time weather updates based on user queries (We’ll see a similar example in this notebook). With function calling, LLMs can intelligently select which functions or tools to use in response to a user’s request. This capability allows agents to make autonomous decisions about how to best fulfill a task, enhancing their efficiency and responsiveness. This article will demonstrate how we used the LLM (here, Mistral) to generate arguments for the defined function, based on the question asked by the user, specifically: The user asks about the temperature in Delhi, the model extracts the arguments, which the function uses to get the real-time information (here, we’ve set to return a default value for demonstration purposes), and then the LLM generates the answer in simple language for the user.\\xa0 Let’s import the necessary libraries and import the model and tokenizer from huggingface for inference setup. The Model is available here. Providing the huggingface model repository name for mistral 7B In the rapidly evolving world of AI, implementing function calling with Mistral 7B empowers developers to create sophisticated agents capable of seamlessly interacting with external systems and delivering precise, context-aware responses. Here, we’re defining the tools (function/s) whose information the model will have access to, for generating the function arguments based on the user query. Tool is defined below: The prompt template for Mistral needs to be in the specific format below for Mistral. Query (the prompt) to be passed to the model Overall, the user’s query along with the information about the available functions is passed to the LLM, based on which the LLM extracts the arguments from the user’s query for the function to be executed. Output :\\xa0[{“name”: “get_current_temperature”, “arguments”: {“location”: “Delhi, India”, “unit”: “celsius”}}]\\nStep 3:Generating a Unique Tool Call ID (Mistral-Specific)\\nIt is used to uniquely identify and match tool calls with their corresponding responses, ensuring consistency and error handling in complex interactions with external tools\\nimport json\\nimport random\\nimport string\\nimport re\\nGenerate a random tool_call_id\\xa0\\nIt is used to uniquely identify and match tool calls with their corresponding responses, ensuring consistency and error handling in complex interactions with external tools.\\ntool_call_id = \\'\\'.join(random.choices(string.ascii_letters + string.digits, k=9))\\nAppend the tool call to the conversation\\nmessages.append({\"role\": \"assistant\", \"tool_calls\": [{\"type\": \"function\", \"id\": tool_call_id, \"function\": response}]})\\nprint(messages)\\nOutput :\\xa0\\n\\n\\nStep 4: Parsing Response in JSON Format\\ntry :\\n    tool_call = json.loads(response)[0]\\n\\nexcept :\\n    # Step 1: Extract the JSON-like part using regex\\n    json_part = re.search(r\\'\\\\[.*\\\\]\\', response, re.DOTALL).group(0)\\n\\n    # Step 2: Convert it to a list of dictionaries\\n    tool_call = json.loads(json_part)[0]\\n\\ntool_call\\nOutput :\\xa0\\xa0{‘name’: ‘get_current_temperature’,\\xa0 ‘arguments’: {‘location’: ‘Delhi, India’, ‘unit’: ‘celsius’}}\\n[Note] :\\xa0 In some cases, the model may produce some texts as well alongwith the function information and arguments. The ‘except’ block takes care of extracting the exact syntax from the output It is used to uniquely identify and match tool calls with their corresponding responses, ensuring consistency and error handling in complex interactions with external tools Generate a random tool_call_id\\xa0 It is used to uniquely identify and match tool calls with their corresponding responses, ensuring consistency and error handling in complex interactions with external tools. Append the tool call to the conversation Output :\\xa0 Output :\\xa0\\xa0{‘name’: ‘get_current_temperature’,\\xa0 ‘arguments’: {‘location’: ‘Delhi, India’, ‘unit’: ‘celsius’}} Based on the arguments generated by the model, you pass them to the respective function to execute and obtain the results. Preparing the prompt for passing whole information to the model Finally, the model generates the final response based on the entire conversation that starts with the user’s query and shows it to the user. Output: The current temperature in Delhi is 30 degrees Celsius. We built our first agent that can tell us real-time temperature statistics across the globe! Of course, we used a random temperature as a default value, but you can connect it to weather APIs that fetch real-time data. Technically speaking, based on the natural language query by the user, we were able to get the required arguments from the LLM to execute the function, get the results out, and then generate a natural language response by the LLM. What if we wanted to know the other factors like wind speed, humidity, and UV index? :\\xa0 We just need to define the functions for those factors and pass them in the tools\\xa0argument of the chat template. This way, we can build a comprehensive Weather Agent that has access to real-time weather information. A. Function calling in LLMs allows the model to execute predefined functions based on user prompts, enabling structured interactions with external systems or APIs. A. Mistral 7B excels at instruction-following tasks and can autonomously generate function arguments, making it suitable for applications that require real-time data retrieval. A. JSON schemas define the structure of functions used by LLMs, allowing the models to understand and generate necessary arguments for those functions based on user input. A. You can design AI agents to handle various functionalities by defining multiple functions and integrating them into the agent’s toolset. The media shown in this article is not owned by Analytics Vidhya and is used at the Author’s discretion. Explore Generative AI for beginners: create text and images, use top AI tools, learn practical skills, and ethics. Master Large Language Models (LLMs) with this course, offering clear guidance in NLP and model training made simple. This free course guides you on building LLM apps, mastering prompt engineering, and developing chatbots with enterprise data. Explore practical solutions, advanced retrieval strategies, and agentic RAG systems to improve context, relevance, and accuracy in AI-driven applications. Master MS Excel for data analysis with key formulas, functions, and LookUp tools in this comprehensive course. \\nGPT-4 vs. Llama 3.1 – Which Model is Better?  \\nLlama-3.1-Storm-8B: The 8B LLM Powerhouse Surpa...  \\nA Comprehensive Guide to Building Agentic RAG S...  \\nMachine Learning Algorithms  \\n45 Questions to Test a Data Scientist on Basics...  \\n90+ Python Interview Questions and Answers (202...  \\n6 Easy Ways to Access ChatGPT-4 for Free\\xa0  \\nPrompt Engineering: Definition, Examples, Tips ...  \\nWhat is LangChain?  \\nWhat is Retrieval-Augmented Generation (RAG)?  ClearSubmit reply \\n\\n  Δ Write, captivate, and earn accolades and rewards for your work \\nWe use cookies essential for this site to function well. Please click to help us improve its usefulness with additional cookies. Learn about our use of cookies in our Privacy Policy & Cookies Policy.\\n Show details This site uses cookies to ensure that you get the best experience possible. To learn more about how we use cookies, please refer to our Privacy Policy & Cookies Policy. Necessary cookies help make a website usable by enabling basic functions like page navigation and access to secure areas of the website. The website cannot function properly without these cookies. It is needed for personalizing the website. Expiry: Session Type: HTTP This cookie is used to prevent Cross-site request forgery (often abbreviated as CSRF) attacks of the website Expiry: Session Type: HTTPS Preserves the login/logout state of users across the whole site. Expiry: Session Type: HTTPS Preserves users\\' states across page requests. Expiry: Session Type: HTTPS Google One-Tap login adds this g_state cookie to set the user status on how they interact with the One-Tap modal. Expiry: 365 days Type: HTTP Statistic cookies help website owners to understand how visitors interact with websites by collecting and reporting information anonymously. Used by Microsoft Clarity, to store and track visits across websites. Expiry: 1 Year Type: HTTP Used by Microsoft Clarity, Persists the Clarity User ID and preferences, unique to that site, on the browser. This ensures that behavior in subsequent visits to the same site will be attributed to the same user ID. Expiry: 1 Year Type: HTTP Used by Microsoft Clarity, Connects multiple page views by a user into a single Clarity session recording. Expiry: 1 Day Type: HTTP Collects user data is specifically adapted to the user or device. The user can also be followed outside of the loaded website, creating a picture of the visitor\\'s behavior. Expiry: 2 Years Type: HTTP Use to measure the use of the website for internal analytics Expiry: 1 Years Type: HTTP The cookie is set by embedded Microsoft Clarity scripts. The purpose of this cookie is for heatmap and session recording. Expiry: 1 Year Type: HTTP Collected user data is specifically adapted to the user or device. The user can also be followed outside of the loaded website, creating a picture of the visitor\\'s behavior. Expiry: 2 Months Type: HTTP This cookie is installed by Google Analytics. The cookie is used to store information of how visitors use a website and helps in creating an analytics report of how the website is doing. The data collected includes the number of visitors, the source where they have come from, and the pages visited in an anonymous form. Expiry: 399 Days Type: HTTP Used by Google Analytics, to store and count pageviews. Expiry: 399 Days Type: HTTP Used by Google Analytics to collect data on the number of times a user has visited the website as well as dates for the first and most recent visit. Expiry: 1 Day Type: HTTP Used to send data to Google Analytics about the visitor\\'s device and behavior. Tracks the visitor across devices and marketing channels. Expiry: Session Type: PIXEL cookies ensure that requests within a browsing session are made by the user, and not by other sites. Expiry: 6 Months Type: HTTP use the cookie when customers want to make a referral from their gmail contacts; it helps auth the gmail account. Expiry: 2 Years Type: HTTP This cookie is set by DoubleClick (which is owned by Google) to determine if the website visitor\\'s browser supports cookies. Expiry: 1 Year Type: HTTP this is used to send push notification using webengage. Expiry: 1 Year Type: HTTP used by webenage to track auth of webenagage. Expiry: Session Type: HTTP Linkedin sets this cookie to registers statistical data on users\\' behavior on the website for internal analytics. Expiry: 1 Day Type: HTTP Use to maintain an anonymous user session by the server. Expiry: 1 Year Type: HTTP Used as part of the LinkedIn Remember Me feature and is set when a user clicks Remember Me on the device to make it easier for him or her to sign in to that device. Expiry: 1 Year Type: HTTP Used to store information about the time a sync with the lms_analytics cookie took place for users in the Designated Countries. Expiry: 6 Months Type: HTTP Used to store information about the time a sync with the AnalyticsSyncHistory cookie took place for users in the Designated Countries. Expiry: 6 Months Type: HTTP Cookie used for Sign-in with Linkedin and/or to allow for the Linkedin follow feature. Expiry: 6 Months Type: HTTP allow for the Linkedin follow feature. Expiry: 1 Year Type: HTTP often used to identify you, including your name, interests, and previous activity. Expiry: 2 Months Type: HTTP Tracks the time that the previous page took to load Expiry: Session Type: HTTP Used to remember a user\\'s language setting to ensure LinkedIn.com displays in the language selected by the user in their settings Expiry: Session Type: HTTP Tracks percent of page viewed Expiry: Session Type: HTTP Indicates the start of a session for Adobe Experience Cloud Expiry: Session Type: HTTP Provides page name value (URL) for use by Adobe Analytics Expiry: Session Type: HTTP Used to retain and fetch time since last visit in Adobe Analytics Expiry: 6 Months Type: HTTP Remembers a user\\'s display preference/theme setting Expiry: 6 Months Type: HTTP Remembers which users have updated their display / theme preferences Expiry: 6 Months Type: HTTP Preference cookies enable a website to remember information that changes the way the website behaves or looks, like your preferred language or the region that you are in. Marketing cookies are used to track visitors across websites. The intention is to display ads that are relevant and engaging for the individual user and thereby more valuable for publishers and third party advertisers. Used by Google Adsense, to store and track conversions. Expiry: 3 Months Type: HTTP Save certain preferences, for example the number of search results per page or activation of the SafeSearch Filter. Adjusts the ads that appear in Google Search. Expiry: 2 Years Type: HTTP Save certain preferences, for example the number of search results per page or activation of the SafeSearch Filter. Adjusts the ads that appear in Google Search. Expiry: 2 Years Type: HTTP Save certain preferences, for example the number of search results per page or activation of the SafeSearch Filter. Adjusts the ads that appear in Google Search. Expiry: 2 Years Type: HTTP Save certain preferences, for example the number of search results per page or activation of the SafeSearch Filter. Adjusts the ads that appear in Google Search. Expiry: 2 Years Type: HTTP Save certain preferences, for example the number of search results per page or activation of the SafeSearch Filter. Adjusts the ads that appear in Google Search. Expiry: 2 Years Type: HTTP Save certain preferences, for example the number of search results per page or activation of the SafeSearch Filter. Adjusts the ads that appear in Google Search. Expiry: 2 Years Type: HTTP These cookies are used for the purpose of targeted advertising. Expiry: 6 Hours Type: HTTP These cookies are used for the purpose of targeted advertising. Expiry: 1 Month Type: HTTP These cookies are used to gather website statistics, and track conversion rates. Expiry: 1 Month Type: HTTP Aggregate analysis of website visitors Expiry: 6 Months Type: HTTP This cookie is set by Facebook to deliver advertisements when they are on Facebook or a digital platform powered by Facebook advertising after visiting this website. Expiry: 4 Months Type: HTTP Contains a unique browser and user ID, used for targeted advertising. Expiry: 2 Months Type: HTTP Used by LinkedIn to track the use of embedded services. Expiry: 1 Year Type: HTTP Used by LinkedIn for tracking the use of embedded services. Expiry: 1 Day Type: HTTP Used by LinkedIn to track the use of embedded services. Expiry: 6 Months Type: HTTP Use these cookies to assign a unique ID when users visit a website. Expiry: 6 Months Type: HTTP These cookies are set by LinkedIn for advertising purposes, including: tracking visitors so that more relevant ads can be presented, allowing users to use the \\'Apply with LinkedIn\\' or the \\'Sign-in with LinkedIn\\' functions, collecting information about how visitors use the site, etc. Expiry: 6 Months Type: HTTP Used to make a probabilistic match of a user\\'s identity outside the Designated Countries Expiry: 90 Days Type: HTTP Used to collect information for analytics purposes. Expiry: 1 year Type: HTTP Used to store session ID for a users session to ensure that clicks from adverts on the Bing search engine are verified for reporting purposes and for personalisation Expiry: 1 Day Type: HTTP UnclassNameified cookies are cookies that we are in the process of classNameifying, together with the providers of individual cookies. Cookie declaration last updated on 24/03/2023 by Analytics Vidhya. Cookies are small text files that can be used by websites to make a user\\'s experience more efficient. The law states that we can store cookies on your device if they are strictly necessary for the operation of this site. For all other types of cookies, we need your permission. This site uses different types of cookies. Some cookies are placed by third-party services that appear on our pages. Learn more about who we are, how you can contact us, and how we process personal data in our Privacy Policy. \\nTerms & conditions\\n\\n\\n\\nRefund Policy\\n\\n\\n\\nPrivacy Policy\\n\\n\\n\\nCookies Policy\\n© Analytics Vidhya 2024.All rights reserved.\\n  \\n\\nEdit Resend OTP Resend OTP in 45s'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Send Emails**"
      ],
      "metadata": {
        "id": "MPmqDr7ONIJd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the email server\n",
        "server = smtplib.SMTP('smtp.gmail.com', 587)\n",
        "server.starttls()\n",
        "\n",
        "# Define a function to email the specified personnel based on the recent interaction\n",
        "def send_email(subject, body, recipient):\n",
        "    # Construct the email message\n",
        "    msg = MIMEMultipart()\n",
        "    msg['From'] = os.getenv('SENDER EMAIL')\n",
        "    msg['To'] = recipient\n",
        "    msg['Subject'] = subject\n",
        "\n",
        "    # Attach the body of the email\n",
        "    msg.attach(MIMEText(body, 'plain'))\n",
        "\n",
        "    # Send the email\n",
        "    server.login(msg['From'], os.getenv('EMAIL PASSWORD'))\n",
        "    text = msg.as_string()\n",
        "    server.sendmail(msg['From'], msg['To'], text)"
      ],
      "metadata": {
        "trusted": true,
        "id": "cT5q0U-Hln8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Store scraped web pages in a local repository**"
      ],
      "metadata": {
        "id": "1gqeSo7SNWHi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def store_scraped_page(url, text):\n",
        "    # Connect to the SQLite database\n",
        "    conn = sqlite3.connect('/content/scraped_pages.db')\n",
        "    c = conn.cursor()\n",
        "\n",
        "    # Creating the table if it doesn't exist\n",
        "    c.execute('''CREATE TABLE IF NOT EXISTS scraped_pages\n",
        "                 (url text, text text)''')\n",
        "\n",
        "    # Inserting the scraped page into the table\n",
        "    c.execute(\"INSERT INTO scraped_pages VALUES (?, ?)\", (url, text))\n",
        "\n",
        "    # Commiting the changes and close the connection\n",
        "    conn.commit()\n",
        "    conn.close()"
      ],
      "metadata": {
        "trusted": true,
        "id": "B5Z-3o3Cln8X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "store_scraped_page(\"https://www.analyticsvidhya.com/blog/2024/10/function-calling-in-ai-agents-using-mistral-7b/\",\"test\")"
      ],
      "metadata": {
        "id": "3ry09w0M5Y6F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Function to search the local repository for a specific query**"
      ],
      "metadata": {
        "id": "OmaY6iG6Njqe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def search_local_repository(query):\n",
        "    # Connect to the SQLite database\n",
        "    conn = sqlite3.connect('/content/scraped_pages.db')\n",
        "    c = conn.cursor()\n",
        "\n",
        "    # Search the table for the query\n",
        "    c.execute(\"SELECT text FROM scraped_pages WHERE text LIKE ?\", ('%' + query + '%',))\n",
        "\n",
        "    # Fetch the results\n",
        "    results = c.fetchall()\n",
        "\n",
        "    # Close the connection\n",
        "    conn.close()\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "trusted": true,
        "id": "BYE5Uje4ln8Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "search_local_repository(\"test\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "14Tf5rp6tOep",
        "outputId": "ceaf014e-d7dd-4081-abe6-3255cb49774b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('test',), ('test',)]"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence_transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KPAqmxflunjC",
        "outputId": "e92241d5-05f6-44f0-88ff-7cc22ce07df0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence_transformers in /usr/local/lib/python3.10/dist-packages (3.2.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.44.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.66.5)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (2.4.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.5.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.25.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (10.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.1.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (1.26.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2024.9.11)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.19.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence_transformers) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence_transformers) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "model = SentenceTransformer(model_name)\n",
        "\n",
        "# function to build a vector database\n",
        "def build_vector_database():\n",
        "    # Connecting to the SQLite database\n",
        "    conn = sqlite3.connect('/content/scraped_pages.db')\n",
        "    c = conn.cursor()\n",
        "    c.execute('''CREATE TABLE IF NOT EXISTS scraped_pages\n",
        "                 (url text, text text)''')\n",
        "    # Fetching all the scraped pages\n",
        "    c.execute(\"SELECT text FROM scraped_pages\")\n",
        "    results = c.fetchall()\n",
        "    # Creating a vector database\n",
        "    vectors = []\n",
        "    for result in results:\n",
        "        # Converting the result to a string before creating the vector\n",
        "        text = str(result[0])\n",
        "        # Checking if the text is not empty before creating the vector\n",
        "        if text:\n",
        "            # vector = np.array(text)\n",
        "            # vectors.append(vector)\n",
        "            vector = model.encode(text)\n",
        "            vectors.append(vector)\n",
        "    if vectors:\n",
        "      # Converting list of vectors into a NumPy array\n",
        "      vectors_np = np.array(vectors)\n",
        "      # FAISS index creation\n",
        "      index = faiss.IndexFlatL2(vectors_np.shape[1])  # Use the size of the vector's dimension\n",
        "      index.add(vectors_np)  # Add vectors to the FAISS index\n",
        "      faiss.write_index(index, \"vector_database.index\")  # Save the index\n",
        "      print(\"Vector database built and saved to 'vector_database.index'.\")\n",
        "    else:\n",
        "      print(\"No vectors found to build the index.\")\n",
        "\n",
        "    # Check if vectors is empty before proceeding\n",
        "    \"\"\"if vectors and vectors[0].size > 0:\n",
        "        index = faiss.IndexFlatL2(vectors[0].size)  # Use .size for NumPy arrays\n",
        "        index.add(np.array(vectors))\n",
        "        faiss.write_index(index, \"vector_database.index\")\n",
        "    else:\n",
        "        print(\"No vectors found to build the index.\")\"\"\"\n"
      ],
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Or73xoc-ln8a",
        "outputId": "8a61412c-402b-4afb-afde-e758a4638b3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name bigscience/bloom-560m. Creating a new one with mean pooling.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "build_vector_database()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6P4oz7gOo8fi",
        "outputId": "6664cb25-8fc5-4fa7-8bbe-e7d157550d61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vector database built and saved to 'vector_database.index'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Query the vector database**"
      ],
      "metadata": {
        "id": "F51tvCKBOgsL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def query_vector_database(query):\n",
        "    # Loading the index\n",
        "    index = faiss.read_index(\"vector_database.index\")\n",
        "    # Create a query vector\n",
        "    # query_vector = np.array(query)\n",
        "    query_vector = model.encode(query) #Encoding the query using the sentence transformer model\n",
        "    query_vector = query_vector.astype('float32') #Converting to float32\n",
        "    k=5\n",
        "    # Search the index\n",
        "    D, I = index.search(np.array([query_vector]),k)\n",
        "    # Return the results\n",
        "    return D, I"
      ],
      "metadata": {
        "trusted": true,
        "id": "bujbfBGxln8b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_vector_database(\"sample query text\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nf5t__q7we5R",
        "outputId": "e365892d-857b-4033-8ab0-27581b8022ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[4.5036289e+04, 4.5036289e+04, 2.3746169e+05, 3.4028235e+38,\n",
              "         3.4028235e+38]], dtype=float32),\n",
              " array([[ 0,  2,  1, -1, -1]]))"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Chat Agent Main**"
      ],
      "metadata": {
        "id": "WC4LrPXCOyk0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the chat agent's user interface\n",
        "def chat_agent():\n",
        "    print(\"Welcome to the chat agent!\")\n",
        "    while True:\n",
        "        # Get the user's input\n",
        "        user_input = input(\"User: \")\n",
        "        # Checking if the user wants to query the vector database\n",
        "        if \"query\" in user_input.lower():\n",
        "            # Querying the vector database\n",
        "            D, I = query_vector_database(user_input)\n",
        "            # Printing the results\n",
        "            for i in range(len(D[0])):\n",
        "                print(\"Result {}: {}\".format(i+1, D[0][i]))\n",
        "        # Check if the user wants to extract information from the internet\n",
        "        elif \"extract info\" in user_input.lower():\n",
        "            # Get the URL from the user\n",
        "            url = input(\"Enter the URL: \")\n",
        "            # Extract the information from the internet\n",
        "            text = extract_info(url)\n",
        "            # Print the extracted information\n",
        "            print(\"Extracted Info: \", text)\n",
        "            # Store the scraped page in the local repository\n",
        "            store_scraped_page(url, text)\n",
        "        # Check if the user wants to email the specified personnel\n",
        "        elif \"send email\" in user_input.lower():\n",
        "            # Get the subject, body, and recipient from the user\n",
        "            subject = input(\"Enter the subject: \")\n",
        "            body = input(\"Enter the body: \")\n",
        "            recipient = input(\"Enter the recipient's email: \")\n",
        "            # Send the email\n",
        "            send_email(subject, body, recipient)\n",
        "        # Generate a response based on the user's input\n",
        "        else:\n",
        "            response = generate_response(user_input)\n",
        "\n",
        "            # Print the response\n",
        "            print(\"Chat Agent: \", response)\n",
        "\n",
        "            # Add the user's input and the response to the chat buffer\n",
        "            chat_buffer.append(user_input)\n",
        "            chat_buffer.append(response)\n",
        "\n",
        "# Run the chat agent\n",
        "# chat_agent()"
      ],
      "metadata": {
        "trusted": true,
        "id": "_tHHiiHIln8c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q gradio\n"
      ],
      "metadata": {
        "id": "68uwF_tYKNLG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "demo = gr.ChatInterface(\n",
        "    fn=chat_agent,\n",
        "    title=\"Chat Agent\",\n",
        "    description=\"A simple chat agent that can query a vector database, extract information from the internet, and send emails.\",\n",
        ")\n",
        "\n",
        "# Launch the Gradio interface\n",
        "demo.launch(debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "kKoWOHz8KWkq",
        "outputId": "01b3d325-4d52-4d08-ba4d-df755d2d9b77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gradio/components/chatbot.py:228: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gradio/utils.py:980: UserWarning: Expected 0 arguments for function <function chat_agent at 0x79cecc32a440>, received 2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gradio/utils.py:988: UserWarning: Expected maximum 0 arguments for function <function chat_agent at 0x79cecc32a440>, received 2.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://bdb13297227386b3b3.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://bdb13297227386b3b3.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/queueing.py\", line 622, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/route_utils.py\", line 323, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 2014, in process_api\n",
            "    result = await self.call_function(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1565, in call_function\n",
            "    prediction = await fn(*processed_input)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/utils.py\", line 813, in async_wrapper\n",
            "    response = await f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/chat_interface.py\", line 638, in _submit_fn\n",
            "    response = await anyio.to_thread.run_sync(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/to_thread.py\", line 33, in run_sync\n",
            "    return await get_asynclib().run_sync_in_worker_thread(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 877, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 807, in run\n",
            "    result = context.run(func, *args)\n",
            "TypeError: chat_agent() takes 0 positional arguments but 2 were given\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://7e123bdc4cd139d51c.gradio.live\n",
            "Killing tunnel 127.0.0.1:7861 <> https://86dcd9aff35fb8fbe6.gradio.live\n",
            "Killing tunnel 127.0.0.1:7862 <> https://2aa11afeaed163070b.gradio.live\n",
            "Killing tunnel 127.0.0.1:7863 <> https://ad7e93e70d2ffad0ff.gradio.live\n",
            "Killing tunnel 127.0.0.1:7864 <> https://bdb13297227386b3b3.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    }
  ]
}